{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Use NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec으로 문장을 벡터로 변환하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Genism으로 토지 읽어들이고 Word2Vec로 embedding하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utp-16인코딩으로 파일을 열고 글자를 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = codecs.open('../data/BEXX0003.txt','r', encoding='utf-16')\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body > text\")\n",
    "text = body.getText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트를 한 줄씩 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제 1 편 어둠 발 소리 서다 序 1897 년 한가위 까치 들 울타리 안 감나무 와 아침 \n",
      "165704\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "#split by lines\n",
    "lines = text.split(\"\\r\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    # 형태소 분석하기 - 단어의 기본형 사용\n",
    "    poslist = twitter.pos(line, norm=True, stem=True)\n",
    "    r = []\n",
    "    for word in poslist:\n",
    "        # 어미/조사/구두점 등은 대상에서 제외 \n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "    readlines = (\" \".join(r)).strip()\n",
    "    results.append(readlines)\n",
    "    print(readlines[:50])\n",
    "    print(len(readlines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = '../data/toji.origin'\n",
    "with open(origins, 'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 파라메터 설명\n",
    "config = {  \n",
    "    'min_count': 5,  # 등장 횟수가 5 이하인 단어는 무시  \n",
    "    'window' : skip gram에서, 주변에 몇개의 단어까지 볼 것인가. 보통 5단어 정도로 한다  \n",
    "    'hs' : hirachical softmax, negative sampling flag, 모델 복잡도를 개선하기 위해 사용함, 보통 negative sampling의 성능이 더 좋음  \n",
    "    'size': 300,  # 300차원짜리 벡터스페이스에 embedding, 보통 300차원 정도로 한다.  \n",
    "    'sg': 1,  # 0이면 CBOW, 1이면 skip-gram을 사용한다. 보통 skip-gram을 많이 사용  \n",
    "    'batch_words': 10000,  # 사전을 구축할때 한번에 읽을 단어 수  \n",
    "    'iter': 5,  # 보통 딥러닝에서 말하는 epoch과 비슷한, 반복 횟수  \n",
    "    'workers': multiprocessing.cpu_count(),  \n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "data = word2vec.LineSentence(origins)\n",
    "model = word2vec.Word2Vec(data, \n",
    "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "model.save(\"../data/toji.bin\")\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 사용한 language model을 바탕으로 유사단어 검출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(\"../data/toji.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('장차', 0.8618342876434326),\n",
       " ('벼슬길', 0.846946120262146),\n",
       " ('전답', 0.8333130478858948),\n",
       " ('쉬다', 0.8295556902885437),\n",
       " ('조상', 0.8258488178253174),\n",
       " ('망치다', 0.8249940872192383),\n",
       " ('박서방', 0.8247364163398743),\n",
       " ('게다가', 0.8245334029197693),\n",
       " ('경신', 0.8241356015205383),\n",
       " ('사시', 0.822289228439331)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive =[\"땅\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이쯤', 0.9076478481292725),\n",
       " ('생생', 0.9064553380012512),\n",
       " ('손주', 0.9051131010055542),\n",
       " ('골골', 0.9043962359428406),\n",
       " ('소요', 0.9020668864250183),\n",
       " ('벼슬길', 0.9011671543121338),\n",
       " ('사태', 0.9004140496253967),\n",
       " ('심청', 0.8998978734016418),\n",
       " ('판술', 0.8985966444015503),\n",
       " ('경신', 0.8981589078903198)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"땅\",\"어머니\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summerize : Word2Vec을 이용하여 특정 단어의 유의어, 반의어 등 추출 가능  \n",
    "**Word2Vec으로 단어를 linear expression이 가능하다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
